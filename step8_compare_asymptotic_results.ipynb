{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Asymptotic Results Between Agents\n",
    "\n",
    "In this notebook, I plan to compare the asymptotic results of different agents and see which one gets the best result\n",
    "asymptotically. For this, I would decrease alphas according to a schedule until we reach a stable state for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libs\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "# Auto reloading causes the kernel to reload the libraries we have\n",
    "%autoreload 2\n",
    "\n",
    "# usual imports for visualization, etc.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "# make it reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# show plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some initializations\n",
    "\n",
    "from envs import SmartVac\n",
    "from agents import ReinforceAgent, ReinforceWithBaselineAgent, ActorCriticAgent\n",
    "\n",
    "max_episode_steps = 100\n",
    "results_folder = 'res/'\n",
    "figs_folder = 'figs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reinforce_agent(alpha, schedule=(.9,1000)):\n",
    "    AgentClass = ReinforceAgent\n",
    "    \n",
    "    params_str = f'alpha_{alpha}_episodes_{episode_count}'\n",
    "\n",
    "    agent_name = AgentClass.__name__\n",
    "\n",
    "    mult_avgs = []\n",
    "    mult_probs1 = []\n",
    "    mult_probs2 = []\n",
    "    for i_test in range(num_of_tests):\n",
    "        print()\n",
    "        print(i_test + 1, end=' ')\n",
    "\n",
    "        # Initialize the agent\n",
    "        agent = AgentClass(alpha=alpha)\n",
    "\n",
    "        avgs = []\n",
    "        probs1 = []\n",
    "        probs2 = []\n",
    "\n",
    "        episode_rewards = np.zeros(episode_count)\n",
    "        for i_episode in range(episode_count):\n",
    "            done = False\n",
    "            totalReward = 0\n",
    "            \n",
    "            if (i_episode+1) % schedule[1] == 0:\n",
    "                agent.alpha *= schedule[0]\n",
    "\n",
    "            if i_episode >= plot_count and (i_episode % plot_count == 0):\n",
    "                avg = np.average(episode_rewards[i_episode - plot_count:i_episode])\n",
    "                avgs.append(avg)\n",
    "\n",
    "                # deterministic position\n",
    "                env.x = 0\n",
    "                env.y = 1\n",
    "                obs = env.get_obs()\n",
    "                prob = agent.get_action_vals_for_obs(obs)\n",
    "                probs1.append(prob)\n",
    "\n",
    "                # stochastic position\n",
    "                env.x = 1\n",
    "                env.y = 1\n",
    "                obs = env.get_obs()\n",
    "                prob = agent.get_action_vals_for_obs(obs)\n",
    "                probs2.append(prob)\n",
    "\n",
    "                print('#', end='', flush=True)\n",
    "                if len(avgs) % 100 == 0:\n",
    "                    print(i_episode)\n",
    "\n",
    "            obs = env.reset()\n",
    "            action = agent.start(obs)\n",
    "\n",
    "            step = 0\n",
    "            while not done:\n",
    "                obs, reward, done = env.step(action)\n",
    "                action = agent.step(obs, reward, done)\n",
    "                totalReward += reward\n",
    "\n",
    "                step += 1\n",
    "                if step > max_episode_steps:\n",
    "                    done = True\n",
    "\n",
    "            episode_rewards[i_episode] = totalReward\n",
    "            agent.update_for_episode()\n",
    "\n",
    "        mult_avgs.append(avgs)\n",
    "        mult_probs1.append(probs1)\n",
    "        mult_probs2.append(probs2)\n",
    "\n",
    "    avgs = np.mean(np.array(mult_avgs), axis=0)\n",
    "    \n",
    "    results_tmp = f'\\n\\nAgent: {agent_name} \\tAlpha: {alpha}'\n",
    "    results_tmp += f'\\nAverage: \\t\\t{np.mean(avgs):5.3f}'\n",
    "    results_tmp += f'\\nBest {plot_count} Average: \\t{np.max(avgs):5.3f}'\n",
    "    results_tmp += f'\\nLast {plot_count} Average: \\t{avgs[-1]:5.3f}'\n",
    "    print(results_tmp)\n",
    "    \n",
    "    return avgs\n",
    "\n",
    "def run_reinforce_with_baseline_agent(alpha_theta=0.1, alpha_w=0.01, schedule=(.9,1000)):\n",
    "    AgentClass = ReinforceWithBaselineAgent\n",
    "    \n",
    "    params_str = f'alpha_theta_{alpha_theta}_alpha_w_{alpha_w}_episodes_{episode_count}'\n",
    "\n",
    "    agent_name = AgentClass.__name__\n",
    "\n",
    "    mult_avgs = []\n",
    "    mult_probs1 = []\n",
    "    mult_probs2 = []\n",
    "    for i_test in range(num_of_tests):\n",
    "        print()\n",
    "        print(i_test + 1, end=' ')\n",
    "\n",
    "        # Initialize the agent\n",
    "        agent = AgentClass(alpha_w=alpha_w, alpha_theta=alpha_theta)\n",
    "\n",
    "        avgs = []\n",
    "        probs1 = []\n",
    "        probs2 = []\n",
    "\n",
    "        episode_rewards = np.zeros(episode_count)\n",
    "        for i_episode in range(episode_count):\n",
    "            done = False\n",
    "            totalReward = 0\n",
    "            \n",
    "            if (i_episode+1) % schedule[1] == 0:\n",
    "                agent.alpha_w *= schedule[0]\n",
    "                agent.alpha_theta *= schedule[0]\n",
    "\n",
    "            if i_episode >= plot_count and (i_episode % plot_count == 0):\n",
    "                avg = np.average(episode_rewards[i_episode - plot_count:i_episode])\n",
    "                avgs.append(avg)\n",
    "\n",
    "                # deterministic position\n",
    "                env.x = 0\n",
    "                env.y = 1\n",
    "                obs = env.get_obs()\n",
    "                prob = agent.get_action_vals_for_obs(obs)\n",
    "                probs1.append(prob)\n",
    "\n",
    "                # stochastic position\n",
    "                env.x = 1\n",
    "                env.y = 1\n",
    "                obs = env.get_obs()\n",
    "                prob = agent.get_action_vals_for_obs(obs)\n",
    "                probs2.append(prob)\n",
    "\n",
    "                print('#', end='', flush=True)\n",
    "                if len(avgs) % 100 == 0:\n",
    "                    print(i_episode)\n",
    "\n",
    "            obs = env.reset()\n",
    "            action = agent.start(obs)\n",
    "\n",
    "            step = 0\n",
    "            while not done:\n",
    "                obs, reward, done = env.step(action)\n",
    "                action = agent.step(obs, reward, done)\n",
    "                totalReward += reward\n",
    "\n",
    "                step += 1\n",
    "                if step > max_episode_steps:\n",
    "                    done = True\n",
    "\n",
    "            episode_rewards[i_episode] = totalReward\n",
    "            agent.update_for_episode()\n",
    "\n",
    "        mult_avgs.append(avgs)\n",
    "        mult_probs1.append(probs1)\n",
    "        mult_probs2.append(probs2)\n",
    "\n",
    "    avgs = np.mean(np.array(mult_avgs), axis=0)\n",
    "    \n",
    "    results_tmp = f'\\n\\nAgent: {agent_name} \\tAlpha_w: {alpha_w}\\tAlpha_theta: {alpha_theta}'\n",
    "    results_tmp += f'\\nAverage: \\t\\t{np.mean(avgs):5.3f}'\n",
    "    results_tmp += f'\\nBest {plot_count} Average: \\t{np.max(avgs):5.3f}'\n",
    "    results_tmp += f'\\nLast {plot_count} Average: \\t{avgs[-1]:5.3f}'\n",
    "    print(results_tmp)\n",
    "    \n",
    "    return avgs\n",
    "\n",
    "def run_actor_critic_agent(alpha_theta=0.1, alpha_w=0.01, schedule=(.9,1000)):\n",
    "    AgentClass = ActorCriticAgent\n",
    "\n",
    "    params_str = f'alpha_theta_{alpha_theta}_alpha_w_{alpha_w}_episodes_{episode_count}'\n",
    "\n",
    "    agent_name = AgentClass.__name__\n",
    "\n",
    "    mult_avgs = []\n",
    "    mult_probs1 = []\n",
    "    mult_probs2 = []\n",
    "    for i_test in range(num_of_tests):\n",
    "        print()\n",
    "        print(i_test + 1, end=' ')\n",
    "\n",
    "        # Initialize the agent\n",
    "        agent = AgentClass(alpha_w=alpha_w, alpha_theta=alpha_theta)\n",
    "\n",
    "        avgs = []\n",
    "        probs1 = []\n",
    "        probs2 = []\n",
    "\n",
    "        episode_rewards = np.zeros(episode_count)\n",
    "        for i_episode in range(episode_count):\n",
    "            done = False\n",
    "            totalReward = 0\n",
    "            \n",
    "            if (i_episode+1) % schedule[1] == 0:\n",
    "                agent.alpha_w *= schedule[0]\n",
    "                agent.alpha_theta *= schedule[0]\n",
    "\n",
    "            if i_episode >= plot_count and (i_episode % plot_count == 0):\n",
    "                avg = np.average(episode_rewards[i_episode - plot_count:i_episode])\n",
    "                avgs.append(avg)\n",
    "\n",
    "                # deterministic position\n",
    "                env.x = 0\n",
    "                env.y = 1\n",
    "                obs = env.get_obs()\n",
    "                prob = agent.get_action_vals_for_obs(obs)\n",
    "                probs1.append(prob)\n",
    "\n",
    "                # stochastic position\n",
    "                env.x = 1\n",
    "                env.y = 1\n",
    "                obs = env.get_obs()\n",
    "                prob = agent.get_action_vals_for_obs(obs)\n",
    "                probs2.append(prob)\n",
    "\n",
    "                print('#', end='', flush=True)\n",
    "                if len(avgs) % 100 == 0:\n",
    "                    print(i_episode)\n",
    "\n",
    "            obs = env.reset()\n",
    "            action = agent.start(obs)\n",
    "\n",
    "            step = 0\n",
    "            while not done:\n",
    "                obs, reward, done = env.step(action)\n",
    "                action = agent.step(obs, reward, done)\n",
    "                totalReward += reward\n",
    "\n",
    "                step += 1\n",
    "                if step > max_episode_steps:\n",
    "                    done = True\n",
    "\n",
    "            episode_rewards[i_episode] = totalReward\n",
    "            agent.update_for_episode()\n",
    "\n",
    "        mult_avgs.append(avgs)\n",
    "        mult_probs1.append(probs1)\n",
    "        mult_probs2.append(probs2)\n",
    "\n",
    "    avgs = np.mean(np.array(mult_avgs), axis=0)\n",
    "    \n",
    "    results_tmp = f'\\n\\nAgent: {agent_name} \\tAlpha_w: {alpha_w}\\tAlpha_theta: {alpha_theta}'\n",
    "    results_tmp += f'\\nAverage: \\t\\t{np.mean(avgs):5.3f}'\n",
    "    results_tmp += f'\\nBest {plot_count} Average: \\t{np.max(avgs):5.3f}'\n",
    "    results_tmp += f'\\nLast {plot_count} Average: \\t{avgs[-1]:5.3f}'\n",
    "    print(results_tmp)\n",
    "    \n",
    "    return avgs\n",
    "    \n",
    "def plot_agent(avgs, agent_name):\n",
    "    plt.figure(1, figsize=(14,10))\n",
    "    plt.plot(avgs, label=agent_name)\n",
    "    plt.title(f'Average Return in {episode_count} episodes')\n",
    "    plt.xlabel(f'index')\n",
    "    plt.ylabel(f'Average Return per {plot_count} episodes')\n",
    "    plt.axhline(y=best_performance, linewidth=1, color=\"g\", linestyle='--')\n",
    "    # plt.savefig(f'{figs_folder}agent_{agent_name}_{params_str}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 ###################################################################################################\n",
      "2 ###################################################################################################\n",
      "3 ###################################################################################################\n",
      "4 ###################################################################################################\n",
      "5 ###################################################################################################\n",
      "6 ###################################################################################################\n",
      "7 ###################################################################################################\n",
      "8 ###################################################################################################\n",
      "9 ###################################################################################################\n",
      "10 ###################################################################################################\n",
      "\n",
      "Agent: ReinforceAgent \tAlpha: 0.25\n",
      "Average: \t\t0.600\n",
      "Best 100 Average: \t0.636\n",
      "Last 100 Average: \t0.634\n",
      "\n",
      "1 ###################################################################################################\n",
      "2 ###################################################################################################\n",
      "3 ###################################################################################################\n",
      "4 ###################################################################################################\n",
      "5 ###################################################################################################\n",
      "6 ###################################################################################################\n",
      "7 ###################################################################################################\n",
      "8 ###################################################################################################\n",
      "9 ###################################################################################################\n",
      "10 ###################################################################################################\n",
      "\n",
      "Agent: ReinforceWithBaselineAgent \tAlpha_w: 0.25\tAlpha_theta: 0.25\n",
      "Average: \t\t0.590\n",
      "Best 100 Average: \t0.635\n",
      "Last 100 Average: \t0.603\n",
      "\n",
      "1 ###################################################################################################\n",
      "2 ###################################################################################################\n",
      "3 ###################################################################################################\n",
      "4 ###################################################################################################\n",
      "5 ###################################################################################################\n",
      "6 ################################################"
     ]
    }
   ],
   "source": [
    "best_performance = 0.63\n",
    "env = SmartVac()\n",
    "num_of_tests = 10\n",
    "episode_count = 10000\n",
    "plot_count = int(episode_count / 100)\n",
    "\n",
    "avgs1 = run_reinforce_agent(alpha=0.25, schedule=(.9,1000))\n",
    "avgs2 = run_reinforce_with_baseline_agent(alpha_theta=0.25, alpha_w=0.25, schedule=(.9,1000))\n",
    "avgs3 = run_actor_critic_agent(alpha_theta=0.5, alpha_w=0.25, schedule=(.9,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agent(avgs1, 'REINFORCE')\n",
    "plot_agent(avgs2, 'REINFORCE w/ Baseline')\n",
    "plot_agent(avgs3, 'Actor-Critic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Asymptotic Results for other rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_performance = -1.37\n",
    "env = SmartVac(terminal_rewards=(-1,-3))\n",
    "num_of_tests = 1\n",
    "episode_count = 10000\n",
    "plot_count = int(episode_count / 100)\n",
    "\n",
    "avgs1 = run_reinforce_agent(alpha=0.25, schedule=(.9,1000))\n",
    "avgs2 = run_reinforce_with_baseline_agent(alpha_theta=0.25, alpha_w=0.25, schedule=(.9,1000))\n",
    "avgs3 = run_actor_critic_agent(alpha_theta=0.5, alpha_w=0.25, schedule=(.9,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agent(avgs1, 'REINFORCE')\n",
    "plot_agent(avgs2, 'REINFORCE w/ Baseline')\n",
    "plot_agent(avgs3, 'Actor-Critic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
