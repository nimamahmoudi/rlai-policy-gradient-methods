{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will include experimental results on the Actor-Critic agent specified in the RL book by Rich Sutton on the smart vacuum environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import needed libs\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "# Auto reloading causes the kernel to reload the libraries we have\n",
    "%autoreload 2\n",
    "\n",
    "# usual imports for visualization, etc.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "# make it reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# show plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some initializations\n",
    "\n",
    "from envs import SmartVac\n",
    "from agents import ActorCriticAgent\n",
    "\n",
    "max_episode_steps = 10000\n",
    "results_folder = 'res/'\n",
    "figs_folder = 'figs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentClass = ActorCriticAgent\n",
    "\n",
    "best_performance = 0.63\n",
    "env = SmartVac()\n",
    "num_of_tests = 10\n",
    "episode_count = 10000\n",
    "plot_count = int(episode_count / 100)\n",
    "\n",
    "alpha_theta = np.power(2.0, -3)\n",
    "alpha_w = np.power(2.0, -2)\n",
    "params_str = f'alpha_theta_{alpha_theta}_alpha_w_{alpha_w}_episodes_{episode_count}'\n",
    "\n",
    "agent_name = AgentClass.__name__\n",
    "\n",
    "mult_avgs = []\n",
    "mult_probs1 = []\n",
    "mult_probs2 = []\n",
    "for i_test in range(num_of_tests):\n",
    "    print()\n",
    "    print(i_test + 1, end=' ')\n",
    "    \n",
    "    # Initialize the agent\n",
    "    agent = AgentClass(alpha_w=alpha_w, alpha_theta=alpha_theta)\n",
    "    \n",
    "    avgs = []\n",
    "    probs1 = []\n",
    "    probs2 = []\n",
    "        \n",
    "    episode_rewards = np.zeros(episode_count)\n",
    "    for i_episode in range(episode_count):\n",
    "        done = False\n",
    "        totalReward = 0\n",
    "\n",
    "        if i_episode >= plot_count and (i_episode % plot_count == 0):\n",
    "            avg = np.average(episode_rewards[i_episode - plot_count:i_episode])\n",
    "            avgs.append(avg)\n",
    "\n",
    "            # deterministic position\n",
    "            env.x = 0\n",
    "            env.y = 1\n",
    "            obs = env.get_obs()\n",
    "            prob = agent.get_action_vals_for_obs(obs)\n",
    "            probs1.append(prob)\n",
    "\n",
    "            # stochastic position\n",
    "            env.x = 1\n",
    "            env.y = 1\n",
    "            obs = env.get_obs()\n",
    "            prob = agent.get_action_vals_for_obs(obs)\n",
    "            probs2.append(prob)\n",
    "\n",
    "            print('#', end='', flush=True)\n",
    "            if len(avgs) % 100 == 0:\n",
    "                print(i_episode)\n",
    "\n",
    "        obs = env.reset()\n",
    "        action = agent.start(obs)\n",
    "\n",
    "        step = 0\n",
    "        while not done:\n",
    "            obs, reward, done = env.step(action)\n",
    "            action = agent.step(obs, reward, done)\n",
    "            totalReward += reward\n",
    "\n",
    "            step += 1\n",
    "            if step > max_episode_steps:\n",
    "                done = True\n",
    "\n",
    "        episode_rewards[i_episode] = totalReward\n",
    "        agent.update_for_episode()\n",
    "        \n",
    "    mult_avgs.append(avgs)\n",
    "    mult_probs1.append(probs1)\n",
    "    mult_probs2.append(probs2)\n",
    "    \n",
    "avgs = np.mean(np.array(mult_avgs), axis=0)\n",
    "probs1 = np.mean(np.array(mult_probs1), axis=0)\n",
    "probs2 = np.mean(np.array(mult_probs2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,10))\n",
    "plt.plot(avgs)\n",
    "plt.title(f'Average Return in {episode_count} episodes')\n",
    "plt.xlabel(f'index')\n",
    "plt.ylabel(f'Average Return per {plot_count} episodes')\n",
    "plt.axhline(y=best_performance, linewidth=1, color=\"g\", linestyle='--')\n",
    "# plt.savefig(f'{figs_folder}agent_{agent_name}_{params_str}.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2, figsize=(14,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(probs1)\n",
    "plt.title(f'Probs for (0,0) in {episode_count} episodes')\n",
    "plt.xlabel(f'index')\n",
    "plt.ylabel(f'Probability')\n",
    "plt.legend(['UP', 'RIGHT', 'DOWN', 'LEFT'])\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(probs2)\n",
    "plt.title(f'Probs for (0,1) in {episode_count} episodes')\n",
    "plt.xlabel(f'index')\n",
    "plt.ylabel(f'Probability')\n",
    "plt.legend(['UP', 'RIGHT', 'DOWN', 'LEFT'])\n",
    "plt.show()\n",
    "\n",
    "print('')\n",
    "\n",
    "results = f'Average: \\t\\t\\t{np.mean(avgs):5.3f}'\n",
    "results += f'\\nBest {plot_count} Average: \\t{np.max(avgs):5.3f}'\n",
    "results += f'\\nLast {plot_count} Average: \\t{avgs[-1]:5.3f}'\n",
    "\n",
    "print(agent.theta)\n",
    "print(agent.v_hat)\n",
    "results += f'\\n\\nAgent: {agent_name} \\tAlpha_w: {alpha_w}\\tAlpha_theta: {alpha_theta}'\n",
    "\n",
    "test_xs = [0, 1, 2, 3, 4]\n",
    "test_ys = [1, 1, 1, 1, 1]\n",
    "for i in range(len(test_xs)):\n",
    "    env.x = test_xs[i]\n",
    "    env.y = test_ys[i]\n",
    "    obs = env.get_obs()\n",
    "    probs = agent.get_action_vals_for_obs(obs)\n",
    "    results += f'\\nx: {env.x}, y:{env.y}, probs: [{probs[0]:4.2f},{probs[1]:4.2f},{probs[2]:4.2f},{probs[3]:4.2f}]'\n",
    "\n",
    "print(results)\n",
    "# Write to file if needed\n",
    "# file = open(f'{results_folder}agent_{agent_name}_{params_str}.txt', 'w')\n",
    "# file.write(results)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic with other reward values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 ###################################################################################################\n",
      "2 ###################################################################################################\n",
      "3 ###################################################################################################\n",
      "4 ###################################################################################################\n",
      "5 ###################################################################################################\n",
      "6 ###################################################################################################\n",
      "7 ###################################################################################################\n",
      "8 ###################################################################################################\n",
      "9 ###################################################################################################\n",
      "10 ############################################################"
     ]
    }
   ],
   "source": [
    "AgentClass = ActorCriticAgent\n",
    "\n",
    "best_performance = -1.37\n",
    "env = SmartVac(terminal_rewards=(-1,-3))\n",
    "num_of_tests = 10\n",
    "episode_count = 10000\n",
    "plot_count = int(episode_count / 100)\n",
    "\n",
    "alpha_theta = np.power(2.0, -3)\n",
    "alpha_w = np.power(2.0, -2)\n",
    "params_str = f'alpha_theta_{alpha_theta}_alpha_w_{alpha_w}_episodes_{episode_count}'\n",
    "\n",
    "agent_name = AgentClass.__name__\n",
    "\n",
    "mult_avgs = []\n",
    "mult_probs1 = []\n",
    "mult_probs2 = []\n",
    "for i_test in range(num_of_tests):\n",
    "    print()\n",
    "    print(i_test + 1, end=' ')\n",
    "    \n",
    "    # Initialize the agent\n",
    "    agent = AgentClass(alpha_w=alpha_w, alpha_theta=alpha_theta)\n",
    "    \n",
    "    avgs = []\n",
    "    probs1 = []\n",
    "    probs2 = []\n",
    "        \n",
    "    episode_rewards = np.zeros(episode_count)\n",
    "    for i_episode in range(episode_count):\n",
    "        done = False\n",
    "        totalReward = 0\n",
    "\n",
    "        if i_episode >= plot_count and (i_episode % plot_count == 0):\n",
    "            avg = np.average(episode_rewards[i_episode - plot_count:i_episode])\n",
    "            avgs.append(avg)\n",
    "\n",
    "            # deterministic position\n",
    "            env.x = 0\n",
    "            env.y = 1\n",
    "            obs = env.get_obs()\n",
    "            prob = agent.get_action_vals_for_obs(obs)\n",
    "            probs1.append(prob)\n",
    "\n",
    "            # stochastic position\n",
    "            env.x = 1\n",
    "            env.y = 1\n",
    "            obs = env.get_obs()\n",
    "            prob = agent.get_action_vals_for_obs(obs)\n",
    "            probs2.append(prob)\n",
    "\n",
    "            print('#', end='', flush=True)\n",
    "            if len(avgs) % 100 == 0:\n",
    "                print(i_episode)\n",
    "\n",
    "        obs = env.reset()\n",
    "        action = agent.start(obs)\n",
    "\n",
    "        step = 0\n",
    "        while not done:\n",
    "            obs, reward, done = env.step(action)\n",
    "            action = agent.step(obs, reward, done)\n",
    "            totalReward += reward\n",
    "\n",
    "            step += 1\n",
    "            if step > max_episode_steps:\n",
    "                done = True\n",
    "\n",
    "        episode_rewards[i_episode] = totalReward\n",
    "        agent.update_for_episode()\n",
    "        \n",
    "    mult_avgs.append(avgs)\n",
    "    mult_probs1.append(probs1)\n",
    "    mult_probs2.append(probs2)\n",
    "    \n",
    "avgs = np.mean(np.array(mult_avgs), axis=0)\n",
    "probs1 = np.mean(np.array(mult_probs1), axis=0)\n",
    "probs2 = np.mean(np.array(mult_probs2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(14,10))\n",
    "plt.plot(avgs)\n",
    "plt.title(f'Average Return in {episode_count} episodes')\n",
    "plt.xlabel(f'index')\n",
    "plt.ylabel(f'Average Return per {plot_count} episodes')\n",
    "plt.axhline(y=best_performance, linewidth=1, color=\"g\", linestyle='--')\n",
    "# plt.savefig(f'{figs_folder}agent_{agent_name}_{params_str}.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2, figsize=(14,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(probs1)\n",
    "plt.title(f'Probs for (0,0) in {episode_count} episodes')\n",
    "plt.xlabel(f'index')\n",
    "plt.ylabel(f'Probability')\n",
    "plt.legend(['UP', 'RIGHT', 'DOWN', 'LEFT'])\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(probs2)\n",
    "plt.title(f'Probs for (0,1) in {episode_count} episodes')\n",
    "plt.xlabel(f'index')\n",
    "plt.ylabel(f'Probability')\n",
    "plt.legend(['UP', 'RIGHT', 'DOWN', 'LEFT'])\n",
    "plt.show()\n",
    "\n",
    "print('')\n",
    "\n",
    "results = f'Average: \\t\\t\\t{np.mean(avgs):5.3f}'\n",
    "results += f'\\nBest {plot_count} Average: \\t{np.max(avgs):5.3f}'\n",
    "results += f'\\nLast {plot_count} Average: \\t{avgs[-1]:5.3f}'\n",
    "\n",
    "# print(agent.theta)\n",
    "# print(agent.v_hat)\n",
    "results += f'\\n\\nAgent: {agent_name} \\tAlpha_w: {alpha_w}\\tAlpha_theta: {alpha_theta}'\n",
    "\n",
    "test_xs = [0, 1, 2, 3, 4]\n",
    "test_ys = [1, 1, 1, 1, 1]\n",
    "for i in range(len(test_xs)):\n",
    "    env.x = test_xs[i]\n",
    "    env.y = test_ys[i]\n",
    "    obs = env.get_obs()\n",
    "    probs = agent.get_action_vals_for_obs(obs)\n",
    "    results += f'\\nx: {env.x}, y:{env.y}, probs: [{probs[0]:4.2f},{probs[1]:4.2f},{probs[2]:4.2f},{probs[3]:4.2f}]'\n",
    "\n",
    "print(results)\n",
    "# Write to file if needed\n",
    "# file = open(f'{results_folder}agent_{agent_name}_{params_str}.txt', 'w')\n",
    "# file.write(results)\n",
    "# file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
